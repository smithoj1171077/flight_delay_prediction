{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de0f3a4",
   "metadata": {},
   "source": [
    "Feature Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3229222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e55268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Users/kezia/Documents/UniMelb/SML/flight_delay_prediction/src/data\"                     # where your CSVs live; change if needed\n",
    "LISTINGS_CSV = \"listings.csv\"\n",
    "REVIEWS_CSV  = \"reviews.csv\"\n",
    "NEIGH_CSV    = \"neighbourhoods.csv\"   # optional\n",
    "CALENDAR_CSV  = \"calendar.csv\"        # optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfd0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning parameter\n",
    "TFIDF_MAX_FEATURES = 20000     # reduce to e.g. 10000 or 5000 if memory constrained\n",
    "SVD_COMPONENTS = 100           # 50,100,200 are common choices\n",
    "TOP_AMENITIES = 30\n",
    "GEO_CLUSTERS = 10\n",
    "CHUNKSIZE = None               # None => read full reviews at once; or set e.g. 200000 for streaming mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to: /Users/kezia/Documents/UniMelb/SML/processed_out\n"
     ]
    }
   ],
   "source": [
    "# Helpers Function\n",
    "def get_writable_outdir(preferred=None):\n",
    "    if preferred is None:\n",
    "        preferred = [\n",
    "            os.path.join(os.getcwd(), \"processed_out\"),\n",
    "            os.path.expanduser(\"~/processed_out\"),\n",
    "            \"/mnt/data\",\n",
    "            tempfile.gettempdir(),\n",
    "            os.getcwd()\n",
    "        ]\n",
    "    for p in preferred:\n",
    "        try:\n",
    "            p = os.path.expanduser(p)\n",
    "            if not os.path.isabs(p):\n",
    "                p = os.path.abspath(p)\n",
    "            os.makedirs(p, exist_ok=True)\n",
    "            if os.access(p, os.W_OK):\n",
    "                return p\n",
    "        except Exception:\n",
    "            continue\n",
    "    # last resort\n",
    "    fallback = tempfile.gettempdir()\n",
    "    return fallback\n",
    "\n",
    "OUT_DIR = get_writable_outdir()\n",
    "PIPE_DIR = os.path.join(OUT_DIR, \"pipeline_joblib\")\n",
    "os.makedirs(PIPE_DIR, exist_ok=True)\n",
    "print(\"Saving outputs to:\", OUT_DIR)\n",
    "\n",
    "def parse_price(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).replace(\",\", \"\").strip()\n",
    "    s = re.sub(r\"[^\\d\\.-]\", \"\", s)\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def clean_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)          # strip HTML tags\n",
    "    s = re.sub(r\"http\\S+\", \" \", s)        # strip URLs\n",
    "    s = re.sub(r\"\\s+\", \" \", s)            # collapse whitespace\n",
    "    return s.strip()\n",
    "\n",
    "def parse_amenities_field(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    s = str(s).strip()\n",
    "    s = s.strip(\"{}[]\")\n",
    "    parts = [p.strip().strip('\"').strip(\"'\") for p in re.split(r',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', s) if p.strip()]\n",
    "    return [p for p in parts if p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907242d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kezia/Documents/UniMelb/SML/flight_delay_prediction/src/data/listings.csv\n"
     ]
    }
   ],
   "source": [
    "# Read listings.csv (with dtype hints and parse_dates)\n",
    "listings_path = os.path.join(DATA_DIR, LISTINGS_CSV)\n",
    "print(listings_path)\n",
    "if not os.path.exists(listings_path):\n",
    "    raise FileNotFoundError(f\"Could not find {listings_path} - put listings.csv in {DATA_DIR} or change DATA_DIR.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a59ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading listings.csv ...\n",
      "Listings shape: (25801, 79)\n"
     ]
    }
   ],
   "source": [
    "# dtype suggestions to save memory\n",
    "dtype_map = {\n",
    "    \"id\": \"Int64\",\n",
    "    \"host_id\": \"Int64\",\n",
    "    \"accommodates\": \"Int64\",\n",
    "    \"beds\": \"float32\",\n",
    "    \"bedrooms\": \"float32\",\n",
    "    \"host_listings_count\": \"Int64\",\n",
    "    \"number_of_reviews\": \"Int64\",\n",
    "    # add others; use \"Int64\" instead of \"int64\" for columns that may have NA\n",
    "}\n",
    "# parse dates that exist\n",
    "date_cols = []\n",
    "for c in [\"host_since\", \"first_review\", \"last_review\", \"calendar_last_scraped\"]:\n",
    "    date_cols.append(c) if c in pd.read_csv(listings_path, nrows=0).columns else None\n",
    "\n",
    "print(\"Reading listings.csv ...\")\n",
    "\n",
    "listings = pd.read_csv(listings_path, dtype=dtype_map, parse_dates=[c for c in date_cols if c], low_memory=False)\n",
    "print(\"Listings shape:\", listings.shape)\n",
    "\n",
    "# unify id column name for merges\n",
    "if \"id\" in listings.columns:\n",
    "    listings = listings.rename(columns={\"id\":\"listing_id\"})\n",
    "\n",
    "# basic conversions\n",
    "if \"price\" in listings.columns:\n",
    "    listings[\"price_clean\"] = listings[\"price\"].map(parse_price)\n",
    "else:\n",
    "    listings[\"price_clean\"] = np.nan\n",
    "\n",
    "# parse amenities list into python list\n",
    "if \"amenities\" in listings.columns:\n",
    "    listings[\"amenity_list\"] = listings[\"amenities\"].apply(parse_amenities_field)\n",
    "else:\n",
    "    listings[\"amenity_list\"] = [[] for _ in range(len(listings))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e596fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings_small columns: ['listing_id', 'price_clean', 'accommodates', 'beds', 'bedrooms', 'bathrooms', 'host_listings_count', 'number_of_reviews', 'reviews_per_month', 'review_scores_rating', 'latitude', 'longitude', 'host_is_superhost', 'first_review', 'last_review']\n",
      "Top amenities: ['Kitchen', 'Smoke alarm', 'Wifi', 'Hangers', 'Hot water', 'Essentials', 'Hair dryer', 'Iron', 'Dishes and silverware', 'TV', 'Washer', 'Shampoo', 'Cooking basics', 'Microwave', 'Bed linens', 'Air conditioning', 'Refrigerator', 'Heating', 'Hot water kettle', 'Dishwasher', 'Toaster', 'Free parking on premises', 'Self check-in', 'Long term stays allowed', 'Dining table', 'Shower gel', 'Dedicated workspace', 'Oven', 'Wine glasses', 'Cleaning products']\n"
     ]
    }
   ],
   "source": [
    "# Precompute listing-level small table used for merging into reviews\n",
    "listing_small_cols = [\n",
    "    \"listing_id\", \"price_clean\", \"accommodates\", \"beds\", \"bedrooms\", \"bathrooms\",\n",
    "    \"host_listings_count\", \"number_of_reviews\", \"reviews_per_month\",\n",
    "    \"review_scores_rating\", \"latitude\", \"longitude\", \"host_is_superhost\", \"first_review\", \"last_review\"\n",
    "]\n",
    "listing_small_cols = [c for c in listing_small_cols if c in listings.columns]\n",
    "listings_small = listings[listing_small_cols].copy().reset_index(drop=True)\n",
    "print(\"listings_small columns:\", listings_small.columns.tolist())\n",
    "\n",
    "# amenities top-K across all listings\n",
    "all_amen = Counter()\n",
    "for lst in listings_small.index:\n",
    "    # use original listings df mapping\n",
    "    amen = listings.loc[listings[\"listing_id\"] == listings_small.loc[lst, \"listing_id\"], \"amenity_list\"]\n",
    "    if len(amen):\n",
    "        all_amen.update(amen.iloc[0])\n",
    "top_amenities = [a for a,_ in all_amen.most_common(TOP_AMENITIES)]\n",
    "print(\"Top amenities:\", top_amenities)\n",
    "\n",
    "# create amenity binaries per listing and merge into listings_small\n",
    "for amen in top_amenities:\n",
    "    listings_small[f\"amenity__{amen}\"] = listings[\"amenity_list\"].apply(lambda L: 1 if amen in L else 0).values[:len(listings_small)]\n",
    "\n",
    "# host_is_superhost to binary\n",
    "if \"host_is_superhost\" in listings_small.columns:\n",
    "    listings_small[\"host_is_superhost_bin\"] = listings_small[\"host_is_superhost\"].map({\"t\":1,\"f\":0}).fillna(0).astype(int)\n",
    "else:\n",
    "    listings_small[\"host_is_superhost_bin\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b11d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading reviews.csv ... (CHUNKSIZE=None)\n",
      "Reviews shape: (940190, 6)\n",
      "Merged reviews+listings shape: (940190, 51)\n",
      "After dropping missing target, rows: 940190\n"
     ]
    }
   ],
   "source": [
    "# Read reviews.csv\n",
    "reviews_path = os.path.join(DATA_DIR, REVIEWS_CSV)\n",
    "if not os.path.exists(reviews_path):\n",
    "    raise FileNotFoundError(f\"Could not find {reviews_path} - put reviews.csv in {DATA_DIR} or change DATA_DIR.\")\n",
    "\n",
    "print(\"Reading reviews.csv ... (CHUNKSIZE=%s)\" % str(CHUNKSIZE))\n",
    "if CHUNKSIZE is None:\n",
    "    # read whole file (fastest if it fits in memory)\n",
    "    reviews = pd.read_csv(reviews_path, parse_dates=[\"date\"], low_memory=False)\n",
    "    print(\"Reviews shape:\", reviews.shape)\n",
    "else:\n",
    "    # chunked read example (if file is too big)\n",
    "    print(\"Using chunked read. Building reviews dataframe from chunks ...\")\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(reviews_path, parse_dates=[\"date\"], chunksize=CHUNKSIZE):\n",
    "        chunks.append(chunk)\n",
    "    reviews = pd.concat(chunks, ignore_index=True)\n",
    "    print(\"Reviews shape (after concat):\", reviews.shape)\n",
    "\n",
    "# merge listing small table into reviews on listing_id\n",
    "reviews = reviews.merge(listings_small, left_on=\"listing_id\", right_on=\"listing_id\", how=\"left\", suffixes=(\"\",\"_lst\"))\n",
    "print(\"Merged reviews+listings shape:\", reviews.shape)\n",
    "\n",
    "# drop reviews with missing target (listing-level review_scores_rating)\n",
    "if \"review_scores_rating\" not in reviews.columns:\n",
    "    raise RuntimeError(\"review_scores_rating not found in listings_small. Ensure listing file contains ratings.\")\n",
    "reviews = reviews[~reviews[\"review_scores_rating\"].isna()].reset_index(drop=True)\n",
    "print(\"After dropping missing target, rows:\", len(reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc910f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & labeling: target = listing-level numeric rating (regression)\n",
    "y = reviews[\"review_scores_rating\"].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315831ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text and computing text stats ...\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning + token stats\n",
    "print(\"Cleaning text and computing text stats ...\")\n",
    "reviews[\"comments_clean\"] = reviews[\"comments\"].apply(clean_text)\n",
    "\n",
    "def text_stats_series(s):\n",
    "    words = s.split()\n",
    "    wc = len(words)\n",
    "    char_count = len(s)\n",
    "    avg_word_len = (sum(len(w) for w in words) / wc) if wc>0 else 0.0\n",
    "    exclam = s.count(\"!\")\n",
    "    question = s.count(\"?\")\n",
    "    uppercase_words = sum(1 for w in words if any(c.isupper() for c in w))\n",
    "    uppercase_ratio = uppercase_words / wc if wc>0 else 0.0\n",
    "    stopwords = sum(1 for w in words if w.lower() in ENGLISH_STOP_WORDS)\n",
    "    stopword_ratio = stopwords / wc if wc>0 else 0.0\n",
    "    return pd.Series({\n",
    "        \"word_count\": wc,\n",
    "        \"char_count\": char_count,\n",
    "        \"avg_word_len\": avg_word_len,\n",
    "        \"exclam_count\": exclam,\n",
    "        \"question_count\": question,\n",
    "        \"uppercase_ratio\": uppercase_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio\n",
    "    })\n",
    "\n",
    "text_stats_df = reviews[\"comments_clean\"].apply(text_stats_series)\n",
    "reviews = pd.concat([reviews.reset_index(drop=True), text_stats_df.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge amenity binaries from listings_small\n",
    "amen_cols = [f\"amenity__{a}\" for a in top_amenities if f\"amenity__{a}\" in listings_small.columns]\n",
    "if amen_cols:\n",
    "    reviews = reviews.merge(listings_small[[\"listing_id\"] + amen_cols], on=\"listing_id\", how=\"left\", suffixes=(\"\",\"_lst\"))\n",
    "    # fill NaN amen columns with 0\n",
    "    for c in amen_cols:\n",
    "        reviews[c] = reviews[c].fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc8db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing numeric features ...\n"
     ]
    }
   ],
   "source": [
    "# Numeric features, price parsing and price_per_person\n",
    "print(\"Constructing numeric features ...\")\n",
    "if \"price_clean\" not in reviews.columns and \"price_clean\" in listings_small.columns:\n",
    "    reviews = reviews.merge(listings_small[[\"listing_id\",\"price_clean\"]], on=\"listing_id\", how=\"left\")\n",
    "\n",
    "numeric_cols = []\n",
    "for c in [\"price_clean\", \"accommodates\", \"beds\", \"bedrooms\", \"bathrooms\", \"host_listings_count\", \"number_of_reviews\", \"reviews_per_month\"]:\n",
    "    if c in reviews.columns:\n",
    "        reviews[f\"{c}_num\"] = pd.to_numeric(reviews[c], errors=\"coerce\")\n",
    "        numeric_cols.append(f\"{c}_num\")\n",
    "\n",
    "if \"price_clean\" in reviews.columns and \"accommodates\" in reviews.columns:\n",
    "    reviews[\"price_per_person\"] = reviews[\"price_clean\"] / reviews[\"accommodates\"].replace(0, pd.NA)\n",
    "    numeric_cols.append(\"price_per_person\")\n",
    "\n",
    "# host superhost binary\n",
    "if \"host_is_superhost\" in reviews.columns:\n",
    "    reviews[\"host_is_superhost_bin\"] = reviews[\"host_is_superhost\"].map({\"t\":1,\"f\":0}).fillna(0).astype(int)\n",
    "    numeric_cols.append(\"host_is_superhost_bin\")\n",
    "else:\n",
    "    reviews[\"host_is_superhost_bin\"] = 0\n",
    "    numeric_cols.append(\"host_is_superhost_bin\")\n",
    "\n",
    "# temporal features from review date\n",
    "reviews[\"review_date\"] = pd.to_datetime(reviews[\"date\"], errors=\"coerce\")\n",
    "max_date = reviews[\"review_date\"].max()\n",
    "reviews[\"review_age_days\"] = (max_date - reviews[\"review_date\"]).dt.days\n",
    "reviews[\"review_month\"] = reviews[\"review_date\"].dt.month\n",
    "reviews[\"review_weekday\"] = reviews[\"review_date\"].dt.weekday\n",
    "reviews[\"review_year\"] = reviews[\"review_date\"].dt.year\n",
    "numeric_cols += [\"review_age_days\", \"review_month\", \"review_weekday\", \"review_year\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geo clustering\n",
    "if \"latitude\" in listings.columns and \"longitude\" in listings.columns and listings[[\"latitude\",\"longitude\"]].dropna().shape[0] >= GEO_CLUSTERS:\n",
    "    coords = listings[[\"latitude\",\"longitude\"]].dropna()\n",
    "    km = KMeans(n_clusters=GEO_CLUSTERS, random_state=42)\n",
    "    km.fit(coords)\n",
    "    # map listing_id -> cluster\n",
    "    listings[\"geo_cluster\"] = -1\n",
    "    listings.loc[coords.index, \"geo_cluster\"] = km.labels_\n",
    "    # merge into reviews\n",
    "    reviews = reviews.merge(listings[[\"listing_id\",\"geo_cluster\"]], on=\"listing_id\", how=\"left\")\n",
    "    # one-hot encode\n",
    "    geo_dummies = pd.get_dummies(reviews[\"geo_cluster\"].fillna(-1).astype(int), prefix=\"geo_cluster\")\n",
    "    reviews = pd.concat([reviews.reset_index(drop=True), geo_dummies.reset_index(drop=True)], axis=1)\n",
    "    geo_cols = [c for c in geo_dummies.columns]\n",
    "else:\n",
    "    geo_cols = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF (max_features=20000) ...\n",
      "TF-IDF shape: (940190, 20000)\n",
      "Applying TruncatedSVD with n_components= 100\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + TruncatedSVD\n",
    "print(\"Building TF-IDF (max_features=%d) ...\" % TFIDF_MAX_FEATURES)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=TFIDF_MAX_FEATURES, stop_words=\"english\", min_df=2)\n",
    "# fit on all comments\n",
    "tfidf_matrix = tfidf.fit_transform(reviews[\"comments_clean\"].fillna(\"\").values)\n",
    "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n",
    "\n",
    "svd_cols = []\n",
    "tfidf_summary_cols = []\n",
    "if tfidf_matrix.shape[1] >= 2 and SVD_COMPONENTS and SVD_COMPONENTS > 0:\n",
    "    n_components = min(SVD_COMPONENTS, tfidf_matrix.shape[1]-1)\n",
    "    print(\"Applying TruncatedSVD with n_components=\", n_components)\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    svd_feats = svd.fit_transform(tfidf_matrix)\n",
    "    svd_cols = [f\"svd_text_{i}\" for i in range(svd_feats.shape[1])]\n",
    "    svd_df = pd.DataFrame(svd_feats, columns=svd_cols, index=reviews.index)\n",
    "    reviews = pd.concat([reviews.reset_index(drop=True), svd_df.reset_index(drop=True)], axis=1)\n",
    "else:\n",
    "    # fallback: aggregated tfidf row stats\n",
    "    print(\"TF-IDF vocabulary small; creating tfidf summary stats instead of SVD.\")\n",
    "    row_sums = np.asarray(tfidf_matrix.sum(axis=1)).ravel()\n",
    "    row_means = np.asarray(tfidf_matrix.mean(axis=1)).ravel()\n",
    "    try:\n",
    "        row_max = np.asarray(tfidf_matrix.max(axis=1).toarray()).ravel()\n",
    "    except Exception:\n",
    "        row_max = np.zeros_like(row_sums)\n",
    "    row_nnz = np.diff(tfidf_matrix.tocsr().indptr)\n",
    "    reviews[\"tfidf_sum\"] = row_sums\n",
    "    reviews[\"tfidf_mean\"] = row_means\n",
    "    reviews[\"tfidf_max\"] = row_max\n",
    "    reviews[\"tfidf_nnz\"] = row_nnz\n",
    "    tfidf_summary_cols = [\"tfidf_sum\",\"tfidf_mean\",\"tfidf_max\",\"tfidf_nnz\"]\n",
    "    svd = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e8c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of engineered features: 161\n"
     ]
    }
   ],
   "source": [
    "# Assemble final feature list\n",
    "text_stat_cols = [\"word_count\",\"char_count\",\"avg_word_len\",\"exclam_count\",\"question_count\",\"uppercase_ratio\",\"stopword_ratio\"]\n",
    "amen_cols = [f\"amenity__{a}\" for a in top_amenities if f\"amenity__{a}\" in reviews.columns]\n",
    "num_cols = [c for c in numeric_cols if c in reviews.columns]\n",
    "svd_cols = [c for c in reviews.columns if c.startswith(\"svd_text_\")]\n",
    "tfidf_summary_cols = [c for c in tfidf_summary_cols if c in reviews.columns]\n",
    "\n",
    "feature_cols = []\n",
    "for group in (text_stat_cols, amen_cols, geo_cols, num_cols, svd_cols, tfidf_summary_cols):\n",
    "    feature_cols += [c for c in group if c in reviews.columns]\n",
    "\n",
    "# drop duplicates if any\n",
    "feature_cols = list(dict.fromkeys(feature_cols))\n",
    "print(\"Number of engineered features:\", len(feature_cols))\n",
    "\n",
    "X = reviews[feature_cols].copy()\n",
    "# coerce all to numeric, create missingness flags \n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa23eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing & scaling features ...\n"
     ]
    }
   ],
   "source": [
    "# Impute & scale\n",
    "print(\"Imputing & scaling features ...\")\n",
    "all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    print(\"Columns with all NaN (will be re-added as zeros):\", all_nan_cols)\n",
    "X_drop = X.drop(columns=all_nan_cols)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X_drop), columns=X_drop.columns, index=X_drop.index)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled_arr = scaler.fit_transform(X_imputed)\n",
    "X_scaled = pd.DataFrame(X_scaled_arr, columns=X_imputed.columns, index=X_imputed.index)\n",
    "\n",
    "# re-add all-NaN cols as zeros\n",
    "for c in all_nan_cols:\n",
    "    X_scaled[c] = 0.0\n",
    "\n",
    "# reorder to original\n",
    "X_scaled = X_scaled[feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed_features.npz and pipeline artifacts to: /Users/kezia/Documents/UniMelb/SML/processed_out\n",
      "Final shapes: X: (940190, 161)  y: (940190,)\n"
     ]
    }
   ],
   "source": [
    "# Save outputs & pipeline objects\n",
    "np.savez_compressed(os.path.join(OUT_DIR, \"processed_features.npz\"), X=X_scaled.values, y=y, feature_cols=np.array(X_scaled.columns))\n",
    "joblib.dump(tfidf, os.path.join(PIPE_DIR, \"tfidf.joblib\"))\n",
    "if 'svd' in locals() and svd is not None:\n",
    "    joblib.dump(svd, os.path.join(PIPE_DIR, \"svd.joblib\"))\n",
    "joblib.dump(imputer, os.path.join(PIPE_DIR, \"imputer.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(PIPE_DIR, \"scaler.joblib\"))\n",
    "joblib.dump(feature_cols, os.path.join(PIPE_DIR, \"feature_cols.joblib\"))\n",
    "\n",
    "print(\"Saved processed_features.npz and pipeline artifacts to:\", OUT_DIR)\n",
    "print(\"Final shapes: X:\", X_scaled.shape, \" y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings shape: (25801, 82)\n",
      "reviews shape: (940190, 214)\n",
      "merged reviews rows: 940190\n",
      "Feature matrix shape: (940190, 161)\n",
      "Target shape: (940190,)\n",
      "\n",
      "Number of engineered features: 161\n",
      "First 30 feature names: ['word_count', 'char_count', 'avg_word_len', 'exclam_count', 'question_count', 'uppercase_ratio', 'stopword_ratio', 'amenity__Kitchen', 'amenity__Smoke alarm', 'amenity__Wifi', 'amenity__Hangers', 'amenity__Hot water', 'amenity__Essentials', 'amenity__Hair dryer', 'amenity__Iron', 'amenity__Dishes and silverware', 'amenity__TV', 'amenity__Washer', 'amenity__Shampoo', 'amenity__Cooking basics', 'amenity__Microwave', 'amenity__Bed linens', 'amenity__Air conditioning', 'amenity__Refrigerator', 'amenity__Heating', 'amenity__Hot water kettle', 'amenity__Dishwasher', 'amenity__Toaster', 'amenity__Free parking on premises', 'amenity__Self check-in']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>exclam_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>stopword_ratio</th>\n",
       "      <th>amenity__Kitchen</th>\n",
       "      <th>amenity__Smoke alarm</th>\n",
       "      <th>amenity__Wifi</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_text_90</th>\n",
       "      <th>svd_text_91</th>\n",
       "      <th>svd_text_92</th>\n",
       "      <th>svd_text_93</th>\n",
       "      <th>svd_text_94</th>\n",
       "      <th>svd_text_95</th>\n",
       "      <th>svd_text_96</th>\n",
       "      <th>svd_text_97</th>\n",
       "      <th>svd_text_98</th>\n",
       "      <th>svd_text_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.360120</td>\n",
       "      <td>0.321937</td>\n",
       "      <td>-0.143360</td>\n",
       "      <td>0.505254</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.069668</td>\n",
       "      <td>0.999518</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262361</td>\n",
       "      <td>0.389755</td>\n",
       "      <td>-0.173751</td>\n",
       "      <td>0.431762</td>\n",
       "      <td>1.120842</td>\n",
       "      <td>0.379482</td>\n",
       "      <td>1.670625</td>\n",
       "      <td>-0.218726</td>\n",
       "      <td>-0.014563</td>\n",
       "      <td>-0.624396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.346454</td>\n",
       "      <td>1.066159</td>\n",
       "      <td>-0.215714</td>\n",
       "      <td>0.505254</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.750644</td>\n",
       "      <td>0.400833</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128746</td>\n",
       "      <td>0.159128</td>\n",
       "      <td>-0.191099</td>\n",
       "      <td>-0.141062</td>\n",
       "      <td>1.977687</td>\n",
       "      <td>-1.765708</td>\n",
       "      <td>1.746953</td>\n",
       "      <td>1.193520</td>\n",
       "      <td>1.512762</td>\n",
       "      <td>1.039548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.186061</td>\n",
       "      <td>0.148984</td>\n",
       "      <td>-0.142385</td>\n",
       "      <td>0.505254</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.470839</td>\n",
       "      <td>0.896388</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.121517</td>\n",
       "      <td>0.278067</td>\n",
       "      <td>-0.070290</td>\n",
       "      <td>-0.119836</td>\n",
       "      <td>-0.157187</td>\n",
       "      <td>-0.098225</td>\n",
       "      <td>-0.181877</td>\n",
       "      <td>0.189352</td>\n",
       "      <td>-0.125347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.684235</td>\n",
       "      <td>-0.642407</td>\n",
       "      <td>0.037788</td>\n",
       "      <td>0.505254</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.522782</td>\n",
       "      <td>-0.177683</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.292225</td>\n",
       "      <td>-0.045886</td>\n",
       "      <td>1.724226</td>\n",
       "      <td>-0.397150</td>\n",
       "      <td>-0.774681</td>\n",
       "      <td>1.418677</td>\n",
       "      <td>-1.731105</td>\n",
       "      <td>-1.534692</td>\n",
       "      <td>-0.055488</td>\n",
       "      <td>-0.035129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.418139</td>\n",
       "      <td>0.400552</td>\n",
       "      <td>-0.133411</td>\n",
       "      <td>3.614245</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.117137</td>\n",
       "      <td>0.605810</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.685737</td>\n",
       "      <td>-0.608923</td>\n",
       "      <td>-0.191403</td>\n",
       "      <td>0.125161</td>\n",
       "      <td>-0.597587</td>\n",
       "      <td>1.304392</td>\n",
       "      <td>-1.256887</td>\n",
       "      <td>0.775842</td>\n",
       "      <td>-0.572558</td>\n",
       "      <td>-0.341312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.766258</td>\n",
       "      <td>0.793627</td>\n",
       "      <td>-0.118205</td>\n",
       "      <td>-0.531076</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.592606</td>\n",
       "      <td>0.516467</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.551490</td>\n",
       "      <td>-0.148464</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>-0.295961</td>\n",
       "      <td>0.669915</td>\n",
       "      <td>-0.945143</td>\n",
       "      <td>-0.851717</td>\n",
       "      <td>0.518326</td>\n",
       "      <td>-0.748017</td>\n",
       "      <td>-0.466540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.186061</td>\n",
       "      <td>0.196153</td>\n",
       "      <td>-0.114901</td>\n",
       "      <td>0.505254</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.089930</td>\n",
       "      <td>0.441973</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409728</td>\n",
       "      <td>-1.343645</td>\n",
       "      <td>-0.426292</td>\n",
       "      <td>-0.412930</td>\n",
       "      <td>-0.247368</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>-1.306192</td>\n",
       "      <td>-0.232358</td>\n",
       "      <td>0.507033</td>\n",
       "      <td>-0.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.998336</td>\n",
       "      <td>0.940375</td>\n",
       "      <td>-0.150927</td>\n",
       "      <td>1.541585</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.354164</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256621</td>\n",
       "      <td>-0.915412</td>\n",
       "      <td>-0.476128</td>\n",
       "      <td>-0.355721</td>\n",
       "      <td>-0.517972</td>\n",
       "      <td>-0.892206</td>\n",
       "      <td>-0.465257</td>\n",
       "      <td>1.262762</td>\n",
       "      <td>0.213512</td>\n",
       "      <td>-0.070285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.360120</td>\n",
       "      <td>0.311455</td>\n",
       "      <td>-0.148688</td>\n",
       "      <td>1.541585</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.568093</td>\n",
       "      <td>-0.057561</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.739884</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>-0.422960</td>\n",
       "      <td>-0.585370</td>\n",
       "      <td>0.304679</td>\n",
       "      <td>0.370399</td>\n",
       "      <td>-0.162230</td>\n",
       "      <td>0.796518</td>\n",
       "      <td>0.669663</td>\n",
       "      <td>0.025875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.336116</td>\n",
       "      <td>-0.422285</td>\n",
       "      <td>-0.190848</td>\n",
       "      <td>0.505254</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>-0.214139</td>\n",
       "      <td>-0.005856</td>\n",
       "      <td>0.277494</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-2.517791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>-1.006217</td>\n",
       "      <td>-1.483212</td>\n",
       "      <td>-0.681329</td>\n",
       "      <td>-0.440524</td>\n",
       "      <td>-0.967513</td>\n",
       "      <td>0.273685</td>\n",
       "      <td>1.682137</td>\n",
       "      <td>0.416893</td>\n",
       "      <td>-0.018243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  char_count  avg_word_len  exclam_count  question_count  \\\n",
       "0    0.360120    0.321937     -0.143360      0.505254       -0.046228   \n",
       "1    1.346454    1.066159     -0.215714      0.505254       -0.046228   \n",
       "2    0.186061    0.148984     -0.142385      0.505254       -0.046228   \n",
       "3   -0.684235   -0.642407      0.037788      0.505254       -0.046228   \n",
       "4    0.418139    0.400552     -0.133411      3.614245       -0.046228   \n",
       "5    0.766258    0.793627     -0.118205     -0.531076       -0.046228   \n",
       "6    0.186061    0.196153     -0.114901      0.505254       -0.046228   \n",
       "7    0.998336    0.940375     -0.150927      1.541585       -0.046228   \n",
       "8    0.360120    0.311455     -0.148688      1.541585       -0.046228   \n",
       "9   -0.336116   -0.422285     -0.190848      0.505254       -0.046228   \n",
       "\n",
       "   uppercase_ratio  stopword_ratio  amenity__Kitchen  amenity__Smoke alarm  \\\n",
       "0        -0.069668        0.999518          0.277494              0.147235   \n",
       "1        -0.750644        0.400833          0.277494              0.147235   \n",
       "2        -0.470839        0.896388          0.277494              0.147235   \n",
       "3        -0.522782       -0.177683          0.277494              0.147235   \n",
       "4        -0.117137        0.605810          0.277494              0.147235   \n",
       "5        -0.592606        0.516467          0.277494              0.147235   \n",
       "6        -0.089930        0.441973          0.277494              0.147235   \n",
       "7         0.012198        0.354164          0.277494              0.147235   \n",
       "8        -0.568093       -0.057561          0.277494              0.147235   \n",
       "9        -0.214139       -0.005856          0.277494              0.147235   \n",
       "\n",
       "   amenity__Wifi  ...  svd_text_90  svd_text_91  svd_text_92  svd_text_93  \\\n",
       "0      -2.517791  ...    -0.262361     0.389755    -0.173751     0.431762   \n",
       "1      -2.517791  ...     0.128746     0.159128    -0.191099    -0.141062   \n",
       "2      -2.517791  ...    -0.056136    -0.121517     0.278067    -0.070290   \n",
       "3      -2.517791  ...    -1.292225    -0.045886     1.724226    -0.397150   \n",
       "4      -2.517791  ...    -0.685737    -0.608923    -0.191403     0.125161   \n",
       "5      -2.517791  ...    -0.551490    -0.148464    -0.001332    -0.295961   \n",
       "6      -2.517791  ...     0.409728    -1.343645    -0.426292    -0.412930   \n",
       "7      -2.517791  ...     1.256621    -0.915412    -0.476128    -0.355721   \n",
       "8      -2.517791  ...    -0.739884    -0.042524    -0.422960    -0.585370   \n",
       "9      -2.517791  ...     0.101530    -1.006217    -1.483212    -0.681329   \n",
       "\n",
       "   svd_text_94  svd_text_95  svd_text_96  svd_text_97  svd_text_98  \\\n",
       "0     1.120842     0.379482     1.670625    -0.218726    -0.014563   \n",
       "1     1.977687    -1.765708     1.746953     1.193520     1.512762   \n",
       "2    -0.119836    -0.157187    -0.098225    -0.181877     0.189352   \n",
       "3    -0.774681     1.418677    -1.731105    -1.534692    -0.055488   \n",
       "4    -0.597587     1.304392    -1.256887     0.775842    -0.572558   \n",
       "5     0.669915    -0.945143    -0.851717     0.518326    -0.748017   \n",
       "6    -0.247368     0.020862    -1.306192    -0.232358     0.507033   \n",
       "7    -0.517972    -0.892206    -0.465257     1.262762     0.213512   \n",
       "8     0.304679     0.370399    -0.162230     0.796518     0.669663   \n",
       "9    -0.440524    -0.967513     0.273685     1.682137     0.416893   \n",
       "\n",
       "   svd_text_99  \n",
       "0    -0.624396  \n",
       "1     1.039548  \n",
       "2    -0.125347  \n",
       "3    -0.035129  \n",
       "4    -0.341312  \n",
       "5    -0.466540  \n",
       "6    -0.377759  \n",
       "7    -0.070285  \n",
       "8     0.025875  \n",
       "9    -0.018243  \n",
       "\n",
       "[10 rows x 161 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target (review_scores_rating) stats:\n",
      "min, median, mean, max: 1.0 4.85 4.806347887129196 5.0\n",
      "count    940190.000000\n",
      "mean          4.806348\n",
      "std           0.179606\n",
      "min           1.000000\n",
      "25%           4.740000\n",
      "50%           4.850000\n",
      "75%           4.930000\n",
      "max           5.000000\n",
      "dtype: float64\n",
      "\n",
      "Sample raw comments and cleaned text:\n",
      "---\n",
      "RAW: It was very convenient  to stay in Lindsay's appartment in Brunswick for a couple of weeks, not only is Lindsay a great host giving you valuable tips the appartment also is in a very interesting part of Melbourne and the CBD is not far away. Highly r\n",
      "CLEAN: It was very convenient to stay in Lindsay's appartment in Brunswick for a couple of weeks, not only is Lindsay a great host giving you valuable tips the appartment also is in a very interesting part of Melbourne and the CBD is not far away. Highly re\n",
      "---\n",
      "RAW: Perfect isnt enough! Lindsay was the best host we could ever imagine help us a lot to handle our new start in australia. His place have a perfect location few second to the tram and 10 minute to cbd , close to a lot of restaurant and 2 park. The room\n",
      "CLEAN: Perfect isnt enough! Lindsay was the best host we could ever imagine help us a lot to handle our new start in australia. His place have a perfect location few second to the tram and 10 minute to cbd , close to a lot of restaurant and 2 park. The room\n",
      "---\n",
      "RAW: Living with Lindsay was very relaxed. The room is choosen in fantastic colours and the colours of the sheets fits very cool together with it too. Lindsay is a very helpfull person and he helped me a lot organisating my journey! \n",
      "CLEAN: Living with Lindsay was very relaxed. The room is choosen in fantastic colours and the colours of the sheets fits very cool together with it too. Lindsay is a very helpfull person and he helped me a lot organisating my journey!\n",
      "---\n",
      "RAW: Beautiful home, great location very friendly and helpful throughout my stay!\n",
      "CLEAN: Beautiful home, great location very friendly and helpful throughout my stay!\n",
      "---\n",
      "<br/>Thank you Lindsay for providing a apartment. He's friendly and fun to hang out with over a bevie! The location could not have been betters itch the tram at the door step and CERES just around the corner. \n",
      "CLEAN: It was great staying at Lindsay's apartment. He's friendly and fun to hang out with over a bevie! The location could not have been betters itch the tram at the door step and CERES just around the corner. Thank you Lindsay for providing a great spacew\n",
      "---\n",
      "RAW: Lindsay was an excellent host, his apartment is in a great location, only a few metres from the end of the Tram 96 stop. Room was spacious enough, my sleep wasn't disturbed by the passing traffic although I would advise very light sleepers to bring e\n",
      "CLEAN: Lindsay was an excellent host, his apartment is in a great location, only a few metres from the end of the Tram 96 stop. Room was spacious enough, my sleep wasn't disturbed by the passing traffic although I would advise very light sleepers to bring e\n",
      "---\n",
      "RAW: Lovely room and home, and very gracious hosts. Lots of character, and I was made to feel right at home. Very close to tram stop and local cafe's, and walking distance from many restaurants. I would highly recommend staying with Lindsay!\n",
      "CLEAN: Lovely room and home, and very gracious hosts. Lots of character, and I was made to feel right at home. Very close to tram stop and local cafe's, and walking distance from many restaurants. I would highly recommend staying with Lindsay!\n",
      "---\n",
      "RAW: A great room in a great location!  The room is very large with a tram stop right across the road.  It's also all within walking distance to yummy cafes, CERES and Sydney Road, Brunswick and only a few tram stops way from other cool areas like Fitzroy\n",
      "CLEAN: A great room in a great location! The room is very large with a tram stop right across the road. It's also all within walking distance to yummy cafes, CERES and Sydney Road, Brunswick and only a few tram stops way from other cool areas like Fitzroy a\n",
      "\n",
      "TF-IDF vocab size: 20000\n",
      "Top 15 TF-IDF terms by IDF (rarest): [('woy', 10.564183583068528), ('kinloch', 10.423104984808623), ('sangat', 10.41003290324127), ('roisin', 10.384390472627931), ('tamar', 10.384390472627931), ('cathi', 10.384390472627931), ('wow wow', 10.384390472627931), ('leila', 10.347119077830701), ('jinny', 10.334997717298355), ('ravin', 10.334997717298355), ('marcia', 10.32302152625164), ('moses', 10.311187068604637), ('căn', 10.311187068604637), ('dom', 10.299491028841446), ('shiyanti', 10.28793020644037)]\n",
      "SVD components shape: (100, 20000)\n",
      "SVD explained variance sum (approx): 0.18856358318543742\n",
      "\n",
      "Feature statistics (first 8 cols):\n",
      "                     count          mean       std       min       25%  \\\n",
      "word_count        940190.0 -5.855503e-17  1.000001 -1.003343 -0.684235   \n",
      "char_count        940190.0  4.068924e-17  1.000001 -1.040723 -0.673853   \n",
      "avg_word_len      940190.0 -4.537788e-16  1.000001 -0.713443 -0.153702   \n",
      "exclam_count      940190.0 -1.819831e-17  1.000001 -0.531076 -0.531076   \n",
      "question_count    940190.0 -6.658103e-18  1.000001 -0.046228 -0.046228   \n",
      "uppercase_ratio   940190.0 -2.043229e-16  1.000001 -1.232659 -0.500598   \n",
      "stopword_ratio    940190.0 -1.484220e-15  1.000001 -2.435987 -0.365875   \n",
      "amenity__Kitchen  940190.0 -1.305925e-17  1.000001 -3.603686  0.277494   \n",
      "\n",
      "                       50%       75%         max  \n",
      "word_count       -0.278097  0.331110   28.006498  \n",
      "char_count       -0.286019  0.337660   26.296335  \n",
      "avg_word_len     -0.109775 -0.050425   94.567746  \n",
      "exclam_count     -0.531076  0.505254   35.740485  \n",
      "question_count   -0.046228 -0.046228  123.388782  \n",
      "uppercase_ratio  -0.201328  0.220113    6.575993  \n",
      "stopword_ratio    0.281035  0.669181    3.774349  \n",
      "amenity__Kitchen  0.277494  0.277494    0.277494  \n"
     ]
    }
   ],
   "source": [
    "# Check the output data\n",
    "# SAhape checks\n",
    "print(\"listings shape:\", listings.shape)\n",
    "print(\"reviews shape:\", reviews.shape)\n",
    "print(\"merged reviews rows:\", len(reviews))\n",
    "print(\"Feature matrix shape:\", X_scaled.shape)   # X_scaled produced by preprocessing\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Feature names & preview\n",
    "print(\"\\nNumber of engineered features:\", len(X_scaled.columns))\n",
    "print(\"First 30 feature names:\", list(X_scaled.columns)[:30])\n",
    "display(X_scaled.head(10))   # Jupyter-friendly\n",
    "\n",
    "# Target distribution and stats\n",
    "import numpy as np\n",
    "print(\"\\nTarget (review_scores_rating) stats:\")\n",
    "print(\"min, median, mean, max:\", np.min(y), np.median(y), np.mean(y), np.max(y))\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(pd.Series(y).describe())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Sample raw and cleaned text\n",
    "print(\"\\nSample raw comments and cleaned text:\")\n",
    "for i,row in reviews[[\"comments\",\"comments_clean\"]].dropna().head(8).iterrows():\n",
    "    print(\"---\")\n",
    "    print(\"RAW:\", row[\"comments\"][:250])\n",
    "    print(\"CLEAN:\", row[\"comments_clean\"][:250])\n",
    "\n",
    "# TF-IDF / SVD quick summary\n",
    "if 'tfidf' in globals():\n",
    "    try:\n",
    "        feat_names = tfidf.get_feature_names_out()\n",
    "        print(\"\\nTF-IDF vocab size:\", len(feat_names))\n",
    "        # top 15 terms by idf (rarest terms)\n",
    "        idf = np.array(tfidf.idf_)\n",
    "        top_idx = idf.argsort()[::-1][:15]\n",
    "        top_terms = [(feat_names[i], idf[i]) for i in top_idx]\n",
    "        print(\"Top 15 TF-IDF terms by IDF (rarest):\", top_terms)\n",
    "    except Exception as e:\n",
    "        print(\"TF-IDF inspection failed:\", e)\n",
    "\n",
    "if 'svd' in globals() and svd is not None:\n",
    "    try:\n",
    "        print(\"SVD components shape:\", svd.components_.shape)\n",
    "        # explained variance if available\n",
    "        if hasattr(svd, \"explained_variance_ratio_\"):\n",
    "            print(\"SVD explained variance sum (approx):\", svd.explained_variance_ratio_.sum())\n",
    "    except Exception as e:\n",
    "        print(\"SVD inspection failed:\", e)\n",
    "\n",
    "# Summary stats of engineered features\n",
    "print(\"\\nFeature statistics (first 8 cols):\")\n",
    "print(X_scaled.iloc[:, :8].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596eda2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
